{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadilou/MachineLearning/blob/master/mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SYw6xnQXkGcQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xCnVVDG_j-ya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kFuNYOdAmnRs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_model_fn(features, labels, mode):\n",
        "    input_layer = tf.reshape(features[\"x\"],[-1,28,28,1])\n",
        "    \n",
        "    conv1 =tf.layers.conv2d(\n",
        "    inputs=input_layer,\n",
        "    filters=32,\n",
        "    kernel_size=[5,5],\n",
        "    padding=\"same\",\n",
        "    activation=tf.nn.relu)\n",
        "    pool1= tf.layers.max_pooling2d(inputs=conv1,pool_size=[2,2],strides=2)\n",
        "    \n",
        "    conv2= tf.layers.conv2d(\n",
        "    inputs=pool1,\n",
        "    filters=64,\n",
        "    kernel_size=[5,5],\n",
        "    padding=\"same\",\n",
        "    activation=tf.nn.relu)\n",
        "    \n",
        "    pool2=tf.layers.max_pooling2d(inputs=conv2,pool_size=[2,2],strides=2)\n",
        "    \n",
        "    pool2_flat = tf.reshape(pool2,[-1,7*7*64])\n",
        "    dense = tf.layers.dense(inputs=pool2_flat, units = 1024, activation=tf.nn.relu)\n",
        "    dropout = tf.layers.dropout( inputs= dense, rate= 0.4, training=mode==tf.estimator.ModeKeys.TRAIN )\n",
        "    \n",
        "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
        "    \n",
        "    predictions = {\n",
        "        \"classes\": tf.argmax(input=logits, axis =1),\n",
        "        \"probabilities\": tf.nn.softmax(logits,name=\"soft_tensor\")\n",
        "    }\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, predictions= predictions)\n",
        "    \n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,logits=logits)\n",
        "    \n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "        train_op = optimizer.minimize(\n",
        "            loss=loss,\n",
        "            global_step=tf.train.get_global_step())\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss,train_op=train_op)\n",
        "\n",
        "    eval_metrcis_ops = {\n",
        "        \"accuracy\": tf.metrics.accuracy(\n",
        "        labels=labels, predictions = predictions[\"classes\"])\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "    mode=mode, loss=loss,eval_metric_ops=eval_metric_ops)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f34Zuzxc-eAU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def main(unused_argv):\n",
        "    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "    train_data = mnist.train.images\n",
        "    train_labels = np.asarray(mnist.train.labels, dtype = np.int32)\n",
        "    eval_data = mnist.test.images\n",
        "    eval_labels = np.asarray(mnist.test.labels, dtype = np.int32)\n",
        "    \n",
        "    mnist_classifier = tf.estimator.Estimator(model_fn = cnn_model_fn, model_dir =\"C:/Users/Kayode Hadilou ADJE/AnacondaProjects\")\n",
        "    \n",
        "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
        "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
        "    \n",
        "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x={\"x\": train_data},\n",
        "        y=train_labels,batch_size=100,\n",
        "        num_epochs=None,\n",
        "        shuffle=True)\n",
        "    mnist_classifier.train(\n",
        "      input_fn=train_input_fn,\n",
        "      steps=20000)    \n",
        "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x={\"x\":eval_data},\n",
        "        y = eval_labels,\n",
        "        num_epochs=1,\n",
        "        shuffle=False)\n",
        "    eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
        "    print (eval_results)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "28v95EcpBPZn",
        "colab_type": "code",
        "outputId": "f4702cf5-113e-42f8-e6f2-ea9e3c14e325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\" :\n",
        "    tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-380bff0dab77>:3: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data.\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'C:/Users/Kayode Hadilou ADJE/AnacondaProjects', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000020820B38C88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from C:/Users/Kayode Hadilou ADJE/AnacondaProjects\\model.ckpt-20000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From C:\\Users\\Kayode Hadilou ADJE\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Saving checkpoints for 20000 into C:/Users/Kayode Hadilou ADJE/AnacondaProjects\\model.ckpt.\n",
            "INFO:tensorflow:loss = 0.1458591, step = 20001\n",
            "INFO:tensorflow:global_step/sec: 2.35974\n",
            "INFO:tensorflow:loss = 0.107656114, step = 20101 (42.381 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.76552\n",
            "INFO:tensorflow:loss = 0.094874494, step = 20201 (56.642 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.29704\n",
            "INFO:tensorflow:loss = 0.105877414, step = 20301 (77.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.43507\n",
            "INFO:tensorflow:loss = 0.1261836, step = 20401 (69.628 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.51498\n",
            "INFO:tensorflow:loss = 0.14230217, step = 20501 (66.101 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.56591\n",
            "INFO:tensorflow:loss = 0.09989386, step = 20601 (63.767 sec)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}